{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "b34bf5a5d603446fb453bbed31c4469c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0873a2dee0e44b0fb3e8445d94c27171",
       "IPY_MODEL_410476fa4ac14dd2b78377f4d779402f",
       "IPY_MODEL_09152dbe3c8543aa809aa592afdc53f1"
      ],
      "layout": "IPY_MODEL_e2a91fa6d1514913892c0def2e2ecf78"
     }
    },
    "0873a2dee0e44b0fb3e8445d94c27171": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fd765bcd2a34db49a02839ba1c4f518",
      "placeholder": "​",
      "style": "IPY_MODEL_d9054c34284045eda3546a21bb5fafc3",
      "value": "Epoch:   2%"
     }
    },
    "410476fa4ac14dd2b78377f4d779402f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a92fc95c6a3d496997da90c87e24ba01",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a98272475d940b1b98438c4d02dcc05",
      "value": 2
     }
    },
    "09152dbe3c8543aa809aa592afdc53f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47b6383ed3b64b7e8c7e66d0e7a468d9",
      "placeholder": "​",
      "style": "IPY_MODEL_1cdde5afac8041bf9b75e0e80bd80630",
      "value": " 2/100 [03:14&lt;2:38:57, 97.32s/it, loss=0.0192]"
     }
    },
    "e2a91fa6d1514913892c0def2e2ecf78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fd765bcd2a34db49a02839ba1c4f518": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9054c34284045eda3546a21bb5fafc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a92fc95c6a3d496997da90c87e24ba01": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a98272475d940b1b98438c4d02dcc05": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "47b6383ed3b64b7e8c7e66d0e7a468d9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cdde5afac8041bf9b75e0e80bd80630": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70b4b657d68648d9be98c099d061cd5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c0237e38cfd4143aa423dea26637965",
       "IPY_MODEL_44c486fa8c4241f4a1a245f4e24da769",
       "IPY_MODEL_12ef3c026774405cb91faf18d7fd8afa"
      ],
      "layout": "IPY_MODEL_99c0577d549c4dd4a61d9ec0cf6254d5"
     }
    },
    "8c0237e38cfd4143aa423dea26637965": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5124950790874225b5cca7556f122f9a",
      "placeholder": "​",
      "style": "IPY_MODEL_f609505b750747dcaff8a950131743e9",
      "value": "Batch: 100%"
     }
    },
    "44c486fa8c4241f4a1a245f4e24da769": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c0a27b6addb4c6a977824995548fe90",
      "max": 379,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d14f2c65ef9348348250af2df0f32963",
      "value": 379
     }
    },
    "12ef3c026774405cb91faf18d7fd8afa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb23edb22d624aee9fd55ab05e2a517e",
      "placeholder": "​",
      "style": "IPY_MODEL_a073d659b0f347c9b30c343430d7f6aa",
      "value": " 379/379 [01:37&lt;00:00,  4.40it/s, loss=0.0185]"
     }
    },
    "99c0577d549c4dd4a61d9ec0cf6254d5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "5124950790874225b5cca7556f122f9a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f609505b750747dcaff8a950131743e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c0a27b6addb4c6a977824995548fe90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d14f2c65ef9348348250af2df0f32963": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eb23edb22d624aee9fd55ab05e2a517e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a073d659b0f347c9b30c343430d7f6aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7508fbad995648a39d2a55953d000786": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9009dc5a656c4befbf513f2574df5a7d",
       "IPY_MODEL_479d2f35498b4fbea77057f3f26fd287",
       "IPY_MODEL_16b5928585984a1b81b809717f125489"
      ],
      "layout": "IPY_MODEL_9d436c7ee90349a5966f025d095dd0cc"
     }
    },
    "9009dc5a656c4befbf513f2574df5a7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e503eeff5d94f0e9dc8a3d31190c6a2",
      "placeholder": "​",
      "style": "IPY_MODEL_4abc5a8b257240b99535a6abe5e00ef6",
      "value": "Batch: 100%"
     }
    },
    "479d2f35498b4fbea77057f3f26fd287": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c25264289e3411db29f6f8db936b00f",
      "max": 379,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0ff8cd1b00544caa12cbd809524dd15",
      "value": 379
     }
    },
    "16b5928585984a1b81b809717f125489": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1faac530b70b41a58a8c9c0cf44af69c",
      "placeholder": "​",
      "style": "IPY_MODEL_7cfefcf49a9a426da4e9203ec4debe38",
      "value": " 379/379 [01:37&lt;00:00,  4.57it/s, loss=0.0103]"
     }
    },
    "9d436c7ee90349a5966f025d095dd0cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "3e503eeff5d94f0e9dc8a3d31190c6a2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4abc5a8b257240b99535a6abe5e00ef6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c25264289e3411db29f6f8db936b00f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ff8cd1b00544caa12cbd809524dd15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1faac530b70b41a58a8c9c0cf44af69c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cfefcf49a9a426da4e9203ec4debe38": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f96241543b441fcb13c16d4ca476405": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e6bbb6ce052045ca895b526c620d8847",
       "IPY_MODEL_5c1c61cd14fb4aa8b335684fbf652a29",
       "IPY_MODEL_a5a2bc5dc0b54a4796e574308bb6c4bc"
      ],
      "layout": "IPY_MODEL_febe66177c604395a78be993eeff7c6d"
     }
    },
    "e6bbb6ce052045ca895b526c620d8847": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6de3445a866442aa9cc7d855cb0d0740",
      "placeholder": "​",
      "style": "IPY_MODEL_353cb60a2a73417b81ccbb5e52480ed3",
      "value": "Batch:   3%"
     }
    },
    "5c1c61cd14fb4aa8b335684fbf652a29": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69f323a406004822bf1a0bfd79767c9a",
      "max": 379,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_88a8f5924a1e4a86805e314dd90c17be",
      "value": 11
     }
    },
    "a5a2bc5dc0b54a4796e574308bb6c4bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ada44d21f234712ac5df797c730495a",
      "placeholder": "​",
      "style": "IPY_MODEL_c777ac64366c40ea8a1f63408879c2ab",
      "value": " 11/379 [00:03&lt;01:34,  3.91it/s, loss=0.0256]"
     }
    },
    "febe66177c604395a78be993eeff7c6d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de3445a866442aa9cc7d855cb0d0740": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "353cb60a2a73417b81ccbb5e52480ed3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69f323a406004822bf1a0bfd79767c9a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88a8f5924a1e4a86805e314dd90c17be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9ada44d21f234712ac5df797c730495a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c777ac64366c40ea8a1f63408879c2ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0a9f64f47d34769a47e165291d2c550": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c31ffecd0494e2a8d0a8cefbea5df8a",
       "IPY_MODEL_1d0a57ed8e914d31bcc1c36a56451df7",
       "IPY_MODEL_e1bb86eb510b47d9ac2311926455bf1f"
      ],
      "layout": "IPY_MODEL_182afbf676cd4c0982b90890bbdbeeef"
     }
    },
    "2c31ffecd0494e2a8d0a8cefbea5df8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d4a78f691ec4555bf179e5ef5d828ac",
      "placeholder": "​",
      "style": "IPY_MODEL_1b96a78d8b8542eb830e3c446a0dcf42",
      "value": "Eval PushTImageEnv: "
     }
    },
    "1d0a57ed8e914d31bcc1c36a56451df7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cc800669dfd45a784864dd4a3881daa",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_95dc6c44f97e4e8882a3657bd2fd66fb",
      "value": 200
     }
    },
    "e1bb86eb510b47d9ac2311926455bf1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_333b5148e59e47f3998a5cf5df531baf",
      "placeholder": "​",
      "style": "IPY_MODEL_185ce2207cec4ae1a6ffdecadae23894",
      "value": " 201/? [00:34&lt;00:00,  6.11it/s, reward=0.828]"
     }
    },
    "182afbf676cd4c0982b90890bbdbeeef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d4a78f691ec4555bf179e5ef5d828ac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b96a78d8b8542eb830e3c446a0dcf42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4cc800669dfd45a784864dd4a3881daa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95dc6c44f97e4e8882a3657bd2fd66fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "333b5148e59e47f3998a5cf5df531baf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "185ce2207cec4ae1a6ffdecadae23894": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# @markdown ### **Imports**\n",
    "# diffusion policy import\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import collections\n",
    "import zarr\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# env import\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import pymunk\n",
    "import pymunk.pygame_util\n",
    "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
    "from pymunk.vec2d import Vec2d\n",
    "import shapely.geometry as sg\n",
    "import cv2\n",
    "import skimage.transform as st\n",
    "\n",
    "from IPython.display import Video\n",
    "import gdown\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# %%\n",
    "# @markdown ### **Environment**\n",
    "# @markdown Defines a PyMunk-based Push-T environment `PushTEnv`.\n",
    "# @markdown And it's subclass `PushTImageEnv`.\n",
    "# @markdown\n",
    "# @markdown **Goal**: push the gray T-block into the green area.\n",
    "# @markdown\n",
    "# @markdown Adapted from [Implicit Behavior Cloning](https://implicitbc.github.io/)\n",
    "\n",
    "\n",
    "positive_y_is_up: bool = False\n",
    "\"\"\"Make increasing values of y point upwards.\n",
    "\n",
    "When True::\n",
    "\n",
    "    y\n",
    "    ^\n",
    "    |      . (3, 3)\n",
    "    |\n",
    "    |   . (2, 2)\n",
    "    |\n",
    "    +------ > x\n",
    "\n",
    "When False::\n",
    "\n",
    "    +------ > x\n",
    "    |\n",
    "    |   . (2, 2)\n",
    "    |\n",
    "    |      . (3, 3)\n",
    "    v\n",
    "    y\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def to_pygame(p: Tuple[float, float], surface: pygame.Surface) -> Tuple[int, int]:\n",
    "    \"\"\"Convenience method to convert pymunk coordinates to pygame surface\n",
    "    local coordinates.\n",
    "\n",
    "    Note that in case positive_y_is_up is False, this function won't actually do\n",
    "    anything except converting the point to integers.\n",
    "    \"\"\"\n",
    "    if positive_y_is_up:\n",
    "        return round(p[0]), surface.get_height() - round(p[1])\n",
    "    else:\n",
    "        return round(p[0]), round(p[1])\n",
    "\n",
    "\n",
    "def light_color(color: SpaceDebugColor):\n",
    "    color = np.minimum(1.2 * np.float32([color.r, color.g, color.b, color.a]), np.float32([255]))\n",
    "    color = SpaceDebugColor(r=color[0], g=color[1], b=color[2], a=color[3])\n",
    "    return color\n",
    "\n",
    "\n",
    "class DrawOptions(pymunk.SpaceDebugDrawOptions):\n",
    "    def __init__(self, surface: pygame.Surface) -> None:\n",
    "        \"\"\"Draw a pymunk.Space on a pygame.Surface object.\n",
    "\n",
    "        Typical usage::\n",
    "\n",
    "        >>> import pymunk\n",
    "        >>> surface = pygame.Surface((10,10))\n",
    "        >>> space = pymunk.Space()\n",
    "        >>> options = pymunk.pygame_util.DrawOptions(surface)\n",
    "        >>> space.debug_draw(options)\n",
    "\n",
    "        You can control the color of a shape by setting shape.color to the color\n",
    "        you want it drawn in::\n",
    "\n",
    "        >>> c = pymunk.Circle(None, 10)\n",
    "        >>> c.color = pygame.Color(\"pink\")\n",
    "\n",
    "        See pygame_util.demo.py for a full example\n",
    "\n",
    "        Since pygame uses a coordiante system where y points down (in contrast\n",
    "        to many other cases), you either have to make the physics simulation\n",
    "        with Pymunk also behave in that way, or flip everything when you draw.\n",
    "\n",
    "        The easiest is probably to just make the simulation behave the same\n",
    "        way as Pygame does. In that way all coordinates used are in the same\n",
    "        orientation and easy to reason about::\n",
    "\n",
    "        >>> space = pymunk.Space()\n",
    "        >>> space.gravity = (0, -1000)\n",
    "        >>> body = pymunk.Body()\n",
    "        >>> body.position = (0, 0) # will be positioned in the top left corner\n",
    "        >>> space.debug_draw(options)\n",
    "\n",
    "        To flip the drawing its possible to set the module property\n",
    "        :py:data:`positive_y_is_up` to True. Then the pygame drawing will flip\n",
    "        the simulation upside down before drawing::\n",
    "\n",
    "        >>> positive_y_is_up = True\n",
    "        >>> body = pymunk.Body()\n",
    "        >>> body.position = (0, 0)\n",
    "        >>> # Body will be position in bottom left corner\n",
    "\n",
    "        :Parameters:\n",
    "                surface : pygame.Surface\n",
    "                    Surface that the objects will be drawn on\n",
    "        \"\"\"\n",
    "        self.surface = surface\n",
    "        super(DrawOptions, self).__init__()\n",
    "\n",
    "    def draw_circle(\n",
    "            self,\n",
    "            pos: Vec2d,\n",
    "            angle: float,\n",
    "            radius: float,\n",
    "            outline_color: SpaceDebugColor,\n",
    "            fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        p = to_pygame(pos, self.surface)\n",
    "\n",
    "        pygame.draw.circle(self.surface, fill_color.as_int(), p, round(radius), 0)\n",
    "        pygame.draw.circle(self.surface, light_color(fill_color).as_int(), p, round(radius - 4), 0)\n",
    "\n",
    "        circle_edge = pos + Vec2d(radius, 0).rotated(angle)\n",
    "        p2 = to_pygame(circle_edge, self.surface)\n",
    "        line_r = 2 if radius > 20 else 1\n",
    "        # pygame.draw.lines(self.surface, outline_color.as_int(), False, [p, p2], line_r)\n",
    "\n",
    "    def draw_segment(self, a: Vec2d, b: Vec2d, color: SpaceDebugColor) -> None:\n",
    "        p1 = to_pygame(a, self.surface)\n",
    "        p2 = to_pygame(b, self.surface)\n",
    "\n",
    "        pygame.draw.aalines(self.surface, color.as_int(), False, [p1, p2])\n",
    "\n",
    "    def draw_fat_segment(\n",
    "            self,\n",
    "            a: Tuple[float, float],\n",
    "            b: Tuple[float, float],\n",
    "            radius: float,\n",
    "            outline_color: SpaceDebugColor,\n",
    "            fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        p1 = to_pygame(a, self.surface)\n",
    "        p2 = to_pygame(b, self.surface)\n",
    "\n",
    "        r = round(max(1, radius * 2))\n",
    "        pygame.draw.lines(self.surface, fill_color.as_int(), False, [p1, p2], r)\n",
    "        if r > 2:\n",
    "            orthog = [abs(p2[1] - p1[1]), abs(p2[0] - p1[0])]\n",
    "            if orthog[0] == 0 and orthog[1] == 0:\n",
    "                return\n",
    "            scale = radius / (orthog[0] * orthog[0] + orthog[1] * orthog[1]) ** 0.5\n",
    "            orthog[0] = round(orthog[0] * scale)\n",
    "            orthog[1] = round(orthog[1] * scale)\n",
    "            points = [\n",
    "                (p1[0] - orthog[0], p1[1] - orthog[1]),\n",
    "                (p1[0] + orthog[0], p1[1] + orthog[1]),\n",
    "                (p2[0] + orthog[0], p2[1] + orthog[1]),\n",
    "                (p2[0] - orthog[0], p2[1] - orthog[1]),\n",
    "            ]\n",
    "            pygame.draw.polygon(self.surface, fill_color.as_int(), points)\n",
    "            pygame.draw.circle(\n",
    "                self.surface,\n",
    "                fill_color.as_int(),\n",
    "                (round(p1[0]), round(p1[1])),\n",
    "                round(radius),\n",
    "            )\n",
    "            pygame.draw.circle(\n",
    "                self.surface,\n",
    "                fill_color.as_int(),\n",
    "                (round(p2[0]), round(p2[1])),\n",
    "                round(radius),\n",
    "            )\n",
    "\n",
    "    def draw_polygon(\n",
    "            self,\n",
    "            verts: Sequence[Tuple[float, float]],\n",
    "            radius: float,\n",
    "            outline_color: SpaceDebugColor,\n",
    "            fill_color: SpaceDebugColor,\n",
    "    ) -> None:\n",
    "        ps = [to_pygame(v, self.surface) for v in verts]\n",
    "        ps += [ps[0]]\n",
    "\n",
    "        radius = 2\n",
    "        pygame.draw.polygon(self.surface, light_color(fill_color).as_int(), ps)\n",
    "\n",
    "        if radius > 0:\n",
    "            for i in range(len(verts)):\n",
    "                a = verts[i]\n",
    "                b = verts[(i + 1) % len(verts)]\n",
    "                self.draw_fat_segment(a, b, radius, fill_color, fill_color)\n",
    "\n",
    "    def draw_dot(\n",
    "            self, size: float, pos: Tuple[float, float], color: SpaceDebugColor\n",
    "    ) -> None:\n",
    "        p = to_pygame(pos, self.surface)\n",
    "        pygame.draw.circle(self.surface, color.as_int(), p, round(size), 0)\n",
    "\n",
    "\n",
    "def pymunk_to_shapely(body, shapes):\n",
    "    geoms = list()\n",
    "    for shape in shapes:\n",
    "        if isinstance(shape, pymunk.shapes.Poly):\n",
    "            verts = [body.local_to_world(v) for v in shape.get_vertices()]\n",
    "            verts += [verts[0]]\n",
    "            geoms.append(sg.Polygon(verts))\n",
    "        else:\n",
    "            raise RuntimeError(f'Unsupported shape type {type(shape)}')\n",
    "    geom = sg.MultiPolygon(geoms)\n",
    "    return geom\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# env\n",
    "class PushTEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n",
    "    reward_range = (0., 1.)\n",
    "\n",
    "    def __init__(self,\n",
    "                 legacy=False,\n",
    "                 block_cog=None, damping=None,\n",
    "                 render_action=True,\n",
    "                 render_size=96,\n",
    "                 reset_to_state=None\n",
    "                 ):\n",
    "        self._seed = None\n",
    "        self.seed()\n",
    "        self.window_size = ws = 512  # The size of the PyGame window\n",
    "        self.render_size = render_size\n",
    "        self.sim_hz = 100\n",
    "        # Local controller params.\n",
    "        self.k_p, self.k_v = 100, 20  # PD control.z\n",
    "        self.control_hz = self.metadata['video.frames_per_second']\n",
    "        # legcay set_state for data compatiblity\n",
    "        self.legacy = legacy\n",
    "\n",
    "        # agent_pos, block_pos, block_angle\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0, 0], dtype=np.float64),\n",
    "            high=np.array([ws, ws, ws, ws, np.pi * 2], dtype=np.float64),\n",
    "            shape=(5,),\n",
    "            dtype=np.float64\n",
    "        )\n",
    "\n",
    "        # positional goal for agent\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0, 0], dtype=np.float64),\n",
    "            high=np.array([ws, ws], dtype=np.float64),\n",
    "            shape=(2,),\n",
    "            dtype=np.float64\n",
    "        )\n",
    "\n",
    "        self.block_cog = block_cog\n",
    "        self.damping = damping\n",
    "        self.render_action = render_action\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.screen = None\n",
    "\n",
    "        self.space = None\n",
    "        self.teleop = None\n",
    "        self.render_buffer = None\n",
    "        self.latest_action = None\n",
    "        self.reset_to_state = reset_to_state\n",
    "\n",
    "    def reset(self):\n",
    "        seed = self._seed\n",
    "        self._setup()\n",
    "        if self.block_cog is not None:\n",
    "            self.block.center_of_gravity = self.block_cog\n",
    "        if self.damping is not None:\n",
    "            self.space.damping = self.damping\n",
    "\n",
    "        # use legacy RandomState for compatiblity\n",
    "        state = self.reset_to_state\n",
    "        if state is None:\n",
    "            rs = np.random.RandomState(seed=seed)\n",
    "            state = np.array([\n",
    "                rs.randint(50, 450), rs.randint(50, 450),\n",
    "                rs.randint(100, 400), rs.randint(100, 400),\n",
    "                rs.randn() * 2 * np.pi - np.pi\n",
    "            ])\n",
    "        self._set_state(state)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        dt = 1.0 / self.sim_hz\n",
    "        self.n_contact_points = 0\n",
    "        n_steps = self.sim_hz // self.control_hz\n",
    "        if action is not None:\n",
    "            self.latest_action = action\n",
    "            for i in range(n_steps):\n",
    "                # Step PD control.\n",
    "                # self.agent.velocity = self.k_p * (act - self.agent.position)    # P control works too.\n",
    "                acceleration = self.k_p * (action - self.agent.position) + self.k_v * (\n",
    "                            Vec2d(0, 0) - self.agent.velocity)\n",
    "                self.agent.velocity += acceleration * dt\n",
    "\n",
    "                # Step physics.\n",
    "                self.space.step(dt)\n",
    "\n",
    "        # compute reward\n",
    "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
    "        goal_geom = pymunk_to_shapely(goal_body, self.block.shapes)\n",
    "        block_geom = pymunk_to_shapely(self.block, self.block.shapes)\n",
    "\n",
    "        intersection_area = goal_geom.intersection(block_geom).area\n",
    "        goal_area = goal_geom.area\n",
    "        coverage = intersection_area / goal_area\n",
    "        reward = np.clip(coverage / self.success_threshold, 0, 1)\n",
    "        done = coverage > self.success_threshold\n",
    "        terminated = done\n",
    "        truncated = done\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode):\n",
    "        return self._render_frame(mode)\n",
    "\n",
    "    def teleop_agent(self):\n",
    "        TeleopAgent = collections.namedtuple('TeleopAgent', ['act'])\n",
    "\n",
    "        def act(obs):\n",
    "            act = None\n",
    "            mouse_position = pymunk.pygame_util.from_pygame(Vec2d(*pygame.mouse.get_pos()), self.screen)\n",
    "            if self.teleop or (mouse_position - self.agent.position).length < 30:\n",
    "                self.teleop = True\n",
    "                act = mouse_position\n",
    "            return act\n",
    "\n",
    "        return TeleopAgent(act)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.array(\n",
    "            tuple(self.agent.position) \\\n",
    "            + tuple(self.block.position) \\\n",
    "            + (self.block.angle % (2 * np.pi),))\n",
    "        return obs\n",
    "\n",
    "    def _get_goal_pose_body(self, pose):\n",
    "        mass = 1\n",
    "        inertia = pymunk.moment_for_box(mass, (50, 100))\n",
    "        body = pymunk.Body(mass, inertia)\n",
    "        # preserving the legacy assignment order for compatibility\n",
    "        # the order here dosn't matter somehow, maybe because CoM is aligned with body origin\n",
    "        body.position = pose[:2].tolist()\n",
    "        body.angle = pose[2]\n",
    "        return body\n",
    "\n",
    "    def _get_info(self):\n",
    "        n_steps = self.sim_hz // self.control_hz\n",
    "        n_contact_points_per_step = int(np.ceil(self.n_contact_points / n_steps))\n",
    "        info = {\n",
    "            'pos_agent': np.array(self.agent.position),\n",
    "            'vel_agent': np.array(self.agent.velocity),\n",
    "            'block_pose': np.array(list(self.block.position) + [self.block.angle]),\n",
    "            'goal_pose': self.goal_pose,\n",
    "            'n_contacts': n_contact_points_per_step}\n",
    "        return info\n",
    "\n",
    "    def _render_frame(self, mode):\n",
    "\n",
    "        if self.window is None and mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        self.screen = canvas\n",
    "\n",
    "        draw_options = DrawOptions(canvas)\n",
    "\n",
    "        # Draw goal pose.\n",
    "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
    "        for shape in self.block.shapes:\n",
    "            goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in\n",
    "                           shape.get_vertices()]\n",
    "            goal_points += [goal_points[0]]\n",
    "            pygame.draw.polygon(canvas, self.goal_color, goal_points)\n",
    "\n",
    "        # Draw agent and block.\n",
    "        self.space.debug_draw(draw_options)\n",
    "\n",
    "        if mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # the clock is aleady ticked during in step for \"human\"\n",
    "\n",
    "        img = np.transpose(\n",
    "            np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "        )\n",
    "        img = cv2.resize(img, (self.render_size, self.render_size))\n",
    "        if self.render_action:\n",
    "            if self.render_action and (self.latest_action is not None):\n",
    "                action = np.array(self.latest_action)\n",
    "                coord = (action / 512 * 96).astype(np.int32)\n",
    "                marker_size = int(8 / 96 * self.render_size)\n",
    "                thickness = int(1 / 96 * self.render_size)\n",
    "                cv2.drawMarker(img, coord,\n",
    "                               color=(255, 0, 0), markerType=cv2.MARKER_CROSS,\n",
    "                               markerSize=marker_size, thickness=thickness)\n",
    "        return img\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        if seed is None:\n",
    "            seed = np.random.randint(0, 25536)\n",
    "        self._seed = seed\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "\n",
    "    def _handle_collision(self, arbiter, space, data):\n",
    "        self.n_contact_points += len(arbiter.contact_point_set.points)\n",
    "\n",
    "    def _set_state(self, state):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = state.tolist()\n",
    "        pos_agent = state[:2]\n",
    "        pos_block = state[2:4]\n",
    "        rot_block = state[4]\n",
    "        self.agent.position = pos_agent\n",
    "        # setting angle rotates with respect to center of mass\n",
    "        # therefore will modify the geometric position\n",
    "        # if not the same as CoM\n",
    "        # therefore should be modified first.\n",
    "        if self.legacy:\n",
    "            # for compatiblity with legacy data\n",
    "            self.block.position = pos_block\n",
    "            self.block.angle = rot_block\n",
    "        else:\n",
    "            self.block.angle = rot_block\n",
    "            self.block.position = pos_block\n",
    "\n",
    "        # Run physics to take effect\n",
    "        self.space.step(1.0 / self.sim_hz)\n",
    "\n",
    "    def _set_state_local(self, state_local):\n",
    "        agent_pos_local = state_local[:2]\n",
    "        block_pose_local = state_local[2:]\n",
    "        tf_img_obj = st.AffineTransform(\n",
    "            translation=self.goal_pose[:2],\n",
    "            rotation=self.goal_pose[2])\n",
    "        tf_obj_new = st.AffineTransform(\n",
    "            translation=block_pose_local[:2],\n",
    "            rotation=block_pose_local[2]\n",
    "        )\n",
    "        tf_img_new = st.AffineTransform(\n",
    "            matrix=tf_img_obj.params @ tf_obj_new.params\n",
    "        )\n",
    "        agent_pos_new = tf_img_new(agent_pos_local)\n",
    "        new_state = np.array(\n",
    "            list(agent_pos_new[0]) + list(tf_img_new.translation) \\\n",
    "            + [tf_img_new.rotation])\n",
    "        self._set_state(new_state)\n",
    "        return new_state\n",
    "\n",
    "    def _setup(self):\n",
    "        self.space = pymunk.Space()\n",
    "        self.space.gravity = 0, 0\n",
    "        self.space.damping = 0\n",
    "        self.teleop = False\n",
    "        self.render_buffer = list()\n",
    "\n",
    "        # Add walls.\n",
    "        walls = [\n",
    "            self._add_segment((5, 506), (5, 5), 2),\n",
    "            self._add_segment((5, 5), (506, 5), 2),\n",
    "            self._add_segment((506, 5), (506, 506), 2),\n",
    "            self._add_segment((5, 506), (506, 506), 2)\n",
    "        ]\n",
    "        self.space.add(*walls)\n",
    "\n",
    "        # Add agent, block, and goal zone.\n",
    "        self.agent = self.add_circle((256, 400), 15)\n",
    "        self.block = self.add_tee((256, 300), 0)\n",
    "        self.goal_color = pygame.Color('LightGreen')\n",
    "        self.goal_pose = np.array([256, 256, np.pi / 4])  # x, y, theta (in radians)\n",
    "\n",
    "        # Add collision handeling\n",
    "        self.collision_handeler = self.space.add_collision_handler(0, 0)\n",
    "        self.collision_handeler.post_solve = self._handle_collision\n",
    "        self.n_contact_points = 0\n",
    "\n",
    "        self.max_score = 50 * 100\n",
    "        self.success_threshold = 0.95  # 95% coverage.\n",
    "\n",
    "    def _add_segment(self, a, b, radius):\n",
    "        shape = pymunk.Segment(self.space.static_body, a, b, radius)\n",
    "        shape.color = pygame.Color('LightGray')  # https://htmlcolorcodes.com/color-names\n",
    "        return shape\n",
    "\n",
    "    def add_circle(self, position, radius):\n",
    "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
    "        body.position = position\n",
    "        body.friction = 1\n",
    "        shape = pymunk.Circle(body, radius)\n",
    "        shape.color = pygame.Color('RoyalBlue')\n",
    "        self.space.add(body, shape)\n",
    "        return body\n",
    "\n",
    "    def add_box(self, position, height, width):\n",
    "        mass = 1\n",
    "        inertia = pymunk.moment_for_box(mass, (height, width))\n",
    "        body = pymunk.Body(mass, inertia)\n",
    "        body.position = position\n",
    "        shape = pymunk.Poly.create_box(body, (height, width))\n",
    "        shape.color = pygame.Color('LightSlateGray')\n",
    "        self.space.add(body, shape)\n",
    "        return body\n",
    "\n",
    "    def add_tee(self, position, angle, scale=30, color='LightSlateGray', mask=pymunk.ShapeFilter.ALL_MASKS()):\n",
    "        mass = 1\n",
    "        length = 4\n",
    "        vertices1 = [(-length * scale / 2, scale),\n",
    "                     (length * scale / 2, scale),\n",
    "                     (length * scale / 2, 0),\n",
    "                     (-length * scale / 2, 0)]\n",
    "        inertia1 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
    "        vertices2 = [(-scale / 2, scale),\n",
    "                     (-scale / 2, length * scale),\n",
    "                     (scale / 2, length * scale),\n",
    "                     (scale / 2, scale)]\n",
    "        inertia2 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
    "        body = pymunk.Body(mass, inertia1 + inertia2)\n",
    "        shape1 = pymunk.Poly(body, vertices1)\n",
    "        shape2 = pymunk.Poly(body, vertices2)\n",
    "        shape1.color = pygame.Color(color)\n",
    "        shape2.color = pygame.Color(color)\n",
    "        shape1.filter = pymunk.ShapeFilter(mask=mask)\n",
    "        shape2.filter = pymunk.ShapeFilter(mask=mask)\n",
    "        body.center_of_gravity = (shape1.center_of_gravity + shape2.center_of_gravity) / 2\n",
    "        body.position = position\n",
    "        body.angle = angle\n",
    "        body.friction = 1\n",
    "        self.space.add(body, shape1, shape2)\n",
    "        return body"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class PushTImageEnv(PushTEnv):\n",
    "    metadata = {\"render.modes\": [\"rgb_array\"], \"video.frames_per_second\": 10}\n",
    "\n",
    "    def __init__(self,\n",
    "                 legacy=False,\n",
    "                 block_cog=None,\n",
    "                 damping=None,\n",
    "                 render_size=96):\n",
    "        super().__init__(\n",
    "            legacy=legacy,\n",
    "            block_cog=block_cog,\n",
    "            damping=damping,\n",
    "            render_size=render_size,\n",
    "            render_action=False)\n",
    "        ws = self.window_size\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'image': spaces.Box(\n",
    "                low=0,\n",
    "                high=1,\n",
    "                shape=(3, render_size, render_size),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'agent_pos': spaces.Box(\n",
    "                low=0,\n",
    "                high=ws,\n",
    "                shape=(2,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        })\n",
    "        self.render_cache = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        img = super()._render_frame(mode='rgb_array')\n",
    "\n",
    "        agent_pos = np.array(self.agent.position)\n",
    "        img_obs = np.moveaxis(img.astype(np.float32) / 255, -1, 0)\n",
    "        obs = {\n",
    "            'image': img_obs,\n",
    "            'agent_pos': agent_pos\n",
    "        }\n",
    "\n",
    "        # draw action\n",
    "        if self.latest_action is not None:\n",
    "            action = np.array(self.latest_action)\n",
    "            coord = (action / 512 * 96).astype(np.int32)\n",
    "            marker_size = int(8 / 96 * self.render_size)\n",
    "            thickness = int(1 / 96 * self.render_size)\n",
    "            cv2.drawMarker(img, coord,\n",
    "                           color=(255, 0, 0), markerType=cv2.MARKER_CROSS,\n",
    "                           markerSize=marker_size, thickness=thickness)\n",
    "        self.render_cache = img\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode):\n",
    "        assert mode == 'rgb_array'\n",
    "\n",
    "        if self.render_cache is None:\n",
    "            self._get_obs()\n",
    "\n",
    "        return self.render_cache\n",
    "\n",
    "\n",
    "# %%\n",
    "# 创建PushTEnv实例\n",
    "env = PushTEnv(\n",
    "    legacy=False,\n",
    "    block_cog=None,\n",
    "    damping=None,\n",
    "    render_action=True,\n",
    "    render_size=96,\n",
    "    reset_to_state=None\n",
    ")\n",
    "\n",
    "# 重置环境\n",
    "obs, info = env.reset()\n",
    "\n",
    "# 打印观察空间和动作空间信息\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Action Space:\", env.action_space)\n",
    "\n",
    "# 定义随机动作\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# 执行动作\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Observation after step:\", obs)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Terminated:\", terminated)\n",
    "print(\"Truncated:\", truncated)\n",
    "print(\"Info:\", info)\n",
    "\n",
    "# 渲染环境\n",
    "env.render(mode=\"human\")\n",
    "\n",
    "# 清理资源\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# @markdown ### **Env Demo**\n",
    "# @markdown Standard Gym Env (0.21.0 API)\n",
    "\n",
    "# 0. create env object\n",
    "env = PushTImageEnv()\n",
    "\n",
    "# 1. seed env for initial state.\n",
    "# Seed 0-200 are used for the demonstration dataset.\n",
    "env.seed(1000)\n",
    "\n",
    "# 2. must reset before use\n",
    "obs, info = env.reset()\n",
    "\n",
    "# 3. 2D positional action space [0,512]\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# 4. Standard gym step method\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# prints and explains each dimension of the observation and action vectors\n",
    "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
    "    print(\"obs['image'].shape:\", obs['image'].shape, \"float32, [0,1]\")\n",
    "    print(\"obs['agent_pos'].shape:\", obs['agent_pos'].shape, \"float32, [0,512]\")\n",
    "    print(\"action.shape: \", action.shape, \"float32, [0,512]\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# @markdown ### **Dataset**\n",
    "# @markdown\n",
    "# @markdown Defines `PushTImageDataset` and helper functions\n",
    "# @markdown\n",
    "# @markdown The dataset class\n",
    "# @markdown - Load data ((image, agent_pos), action) from a zarr storage\n",
    "# @markdown - Normalizes each dimension of agent_pos and action to [-1,1]\n",
    "# @markdown - Returns\n",
    "# @markdown  - All possible segments with length `pred_horizon`\n",
    "# @markdown  - Pads the beginning and the end of each episode with repetition\n",
    "# @markdown  - key `image`: shape (obs_hoirzon, 3, 96, 96)\n",
    "# @markdown  - key `agent_pos`: shape (obs_hoirzon, 2)\n",
    "# @markdown  - key `action`: shape (pred_horizon, 2)\n",
    "\n",
    "def create_sample_indices(\n",
    "        episode_ends: np.ndarray, sequence_length: int,\n",
    "        pad_before: int = 0, pad_after: int = 0):\n",
    "    indices = list()\n",
    "    for i in range(len(episode_ends)):\n",
    "        start_idx = 0\n",
    "        if i > 0:\n",
    "            start_idx = episode_ends[i - 1]\n",
    "        end_idx = episode_ends[i]\n",
    "        episode_length = end_idx - start_idx\n",
    "\n",
    "        min_start = -pad_before\n",
    "        max_start = episode_length - sequence_length + pad_after\n",
    "\n",
    "        # range stops one idx before end\n",
    "        for idx in range(min_start, max_start + 1):\n",
    "            buffer_start_idx = max(idx, 0) + start_idx\n",
    "            buffer_end_idx = min(idx + sequence_length, episode_length) + start_idx\n",
    "            start_offset = buffer_start_idx - (idx + start_idx)\n",
    "            end_offset = (idx + sequence_length + start_idx) - buffer_end_idx\n",
    "            sample_start_idx = 0 + start_offset\n",
    "            sample_end_idx = sequence_length - end_offset\n",
    "            indices.append([\n",
    "                buffer_start_idx, buffer_end_idx,\n",
    "                sample_start_idx, sample_end_idx])\n",
    "    indices = np.array(indices)\n",
    "    return indices\n",
    "\n",
    "\n",
    "def sample_sequence(train_data, sequence_length,\n",
    "                    buffer_start_idx, buffer_end_idx,\n",
    "                    sample_start_idx, sample_end_idx):\n",
    "    result = dict()\n",
    "    for key, input_arr in train_data.items():\n",
    "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
    "        data = sample\n",
    "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
    "            data = np.zeros(\n",
    "                shape=(sequence_length,) + input_arr.shape[1:],\n",
    "                dtype=input_arr.dtype)\n",
    "            if sample_start_idx > 0:\n",
    "                data[:sample_start_idx] = sample[0]\n",
    "            if sample_end_idx < sequence_length:\n",
    "                data[sample_end_idx:] = sample[-1]\n",
    "            data[sample_start_idx:sample_end_idx] = sample\n",
    "        result[key] = data\n",
    "    return result\n",
    "\n",
    "\n",
    "# normalize data\n",
    "def get_data_stats(data):\n",
    "    data = data.reshape(-1, data.shape[-1])\n",
    "    stats = {\n",
    "        'min': np.min(data, axis=0),\n",
    "        'max': np.max(data, axis=0)\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "\n",
    "def normalize_data(data, stats):\n",
    "    # nomalize to [0,1]\n",
    "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
    "    # normalize to [-1, 1]\n",
    "    ndata = ndata * 2 - 1\n",
    "    return ndata\n",
    "\n",
    "\n",
    "def unnormalize_data(ndata, stats):\n",
    "    ndata = (ndata + 1) / 2\n",
    "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
    "    return data\n",
    "\n",
    "\n",
    "# dataset\n",
    "class PushTImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 dataset_path: str,\n",
    "                 pred_horizon: int,\n",
    "                 obs_horizon: int,\n",
    "                 action_horizon: int):\n",
    "        # read from zarr dataset\n",
    "        dataset_root = zarr.open(dataset_path, 'r')\n",
    "\n",
    "        # float32, [0,1], (N,96,96,3)\n",
    "        train_image_data = dataset_root['data']['img'][:]\n",
    "        train_image_data = np.moveaxis(train_image_data, -1, 1)\n",
    "        # (N,3,96,96)\n",
    "\n",
    "        # (N, D)\n",
    "        train_data = {\n",
    "            # first two dims of state vector are agent (i.e. gripper) locations\n",
    "            'agent_pos': dataset_root['data']['state'][:, :2],\n",
    "            'action': dataset_root['data']['action'][:]\n",
    "        }\n",
    "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
    "\n",
    "        # compute start and end of each state-action sequence\n",
    "        # also handles padding\n",
    "        indices = create_sample_indices(\n",
    "            episode_ends=episode_ends,\n",
    "            sequence_length=pred_horizon,\n",
    "            pad_before=obs_horizon - 1,\n",
    "            pad_after=action_horizon - 1)\n",
    "\n",
    "        # compute statistics and normalized data to [-1,1]\n",
    "        stats = dict()\n",
    "        normalized_train_data = dict()\n",
    "        for key, data in train_data.items():\n",
    "            stats[key] = get_data_stats(data)\n",
    "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
    "\n",
    "        # images are already normalized\n",
    "        normalized_train_data['image'] = train_image_data\n",
    "\n",
    "        self.indices = indices\n",
    "        self.stats = stats\n",
    "        self.normalized_train_data = normalized_train_data\n",
    "        self.pred_horizon = pred_horizon\n",
    "        self.action_horizon = action_horizon\n",
    "        self.obs_horizon = obs_horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the start/end indices for this datapoint\n",
    "        buffer_start_idx, buffer_end_idx, \\\n",
    "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
    "\n",
    "        # get nomralized data using these indices\n",
    "        nsample = sample_sequence(\n",
    "            train_data=self.normalized_train_data,\n",
    "            sequence_length=self.pred_horizon,\n",
    "            buffer_start_idx=buffer_start_idx,\n",
    "            buffer_end_idx=buffer_end_idx,\n",
    "            sample_start_idx=sample_start_idx,\n",
    "            sample_end_idx=sample_end_idx\n",
    "        )\n",
    "\n",
    "        # discard unused observations\n",
    "        nsample['image'] = nsample['image'][:self.obs_horizon, :]\n",
    "        nsample['agent_pos'] = nsample['agent_pos'][:self.obs_horizon, :]\n",
    "        return nsample"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# @markdown ### **Dataset Demo**\n",
    "\n",
    "# download demonstration data from Google Drive\n",
    "dataset_path = r\"pusht_cchi_v7_replay.zarr.zip\"\n",
    "# if not os.path.isfile(dataset_path):\n",
    "#     id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
    "#     gdown.download(id=id, output=dataset_path, quiet=False)\n",
    "\n",
    "# parameters\n",
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8\n",
    "# |o|o|                             observations: 2\n",
    "# | |a|a|a|a|a|a|a|a|               actions executed: 8\n",
    "# |p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
    "\n",
    "# create dataset from file\n",
    "dataset = PushTImageDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    pred_horizon=pred_horizon,\n",
    "    obs_horizon=obs_horizon,\n",
    "    action_horizon=action_horizon\n",
    ")\n",
    "# save training data statistics (min, max) for each dim\n",
    "stats = dataset.stats\n",
    "# %%\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    # num_workers=4,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process after each epoch\n",
    "    # persistent_workers=True\n",
    ")\n",
    "\n",
    "# visualize data in batch\n",
    "batch = next(iter(dataloader))\n",
    "print(\"batch['image'].shape:\", batch['image'].shape)\n",
    "print(\"batch['agent_pos'].shape:\", batch['agent_pos'].shape)\n",
    "print(\"batch['action'].shape\", batch['action'].shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# %%\n",
    "# @markdown ### **Network**\n",
    "# @markdown\n",
    "# @markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
    "# @markdown as the noies prediction network\n",
    "# @markdown\n",
    "# @markdown Components\n",
    "# @markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
    "# @markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
    "# @markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
    "# @markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
    "# @markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
    "# @markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
    "# @markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Downsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Upsample1d(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Conv1dBlock(nn.Module):\n",
    "    '''\n",
    "        Conv1d --> GroupNorm --> Mish\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "            nn.GroupNorm(n_groups, out_channels),\n",
    "            nn.Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ConditionalResidualBlock1D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 cond_dim,\n",
    "                 kernel_size=3,\n",
    "                 n_groups=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
    "        ])\n",
    "\n",
    "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
    "        # predicts per-channel scale and bias\n",
    "        cond_channels = out_channels * 2\n",
    "        self.out_channels = out_channels\n",
    "        self.cond_encoder = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(cond_dim, cond_channels),\n",
    "            nn.Unflatten(-1, (-1, 1))\n",
    "        )\n",
    "\n",
    "        # make sure dimensions compatible\n",
    "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        '''\n",
    "            x : [ batch_size x in_channels x horizon ]\n",
    "            cond : [ batch_size x cond_dim]\n",
    "\n",
    "            returns:\n",
    "            out : [ batch_size x out_channels x horizon ]\n",
    "        '''\n",
    "        out = self.blocks[0](x)\n",
    "        embed = self.cond_encoder(cond)\n",
    "\n",
    "        embed = embed.reshape(\n",
    "            embed.shape[0], 2, self.out_channels, 1)\n",
    "        scale = embed[:, 0, ...]\n",
    "        bias = embed[:, 1, ...]\n",
    "        out = scale * out + bias\n",
    "\n",
    "        out = self.blocks[1](out)\n",
    "        out = out + self.residual_conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalUnet1D(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 global_cond_dim,\n",
    "                 diffusion_step_embed_dim=256,\n",
    "                 down_dims=[256, 512, 1024],\n",
    "                 kernel_size=5,\n",
    "                 n_groups=8\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        input_dim: Dim of actions.\n",
    "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
    "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
    "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
    "        down_dims: Channel size for each UNet level.\n",
    "          The length of this array determines numebr of levels.\n",
    "        kernel_size: Conv kernel size\n",
    "        n_groups: Number of groups for GroupNorm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        all_dims = [input_dim] + list(down_dims)\n",
    "        start_dim = down_dims[0]\n",
    "\n",
    "        dsed = diffusion_step_embed_dim\n",
    "        diffusion_step_encoder = nn.Sequential(\n",
    "            SinusoidalPosEmb(dsed),\n",
    "            nn.Linear(dsed, dsed * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(dsed * 4, dsed),\n",
    "        )\n",
    "        cond_dim = dsed + global_cond_dim\n",
    "\n",
    "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
    "        mid_dim = all_dims[-1]\n",
    "        self.mid_modules = nn.ModuleList([\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "            ConditionalResidualBlock1D(\n",
    "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
    "                kernel_size=kernel_size, n_groups=n_groups\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "        down_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            down_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out, dim_out, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        up_modules = nn.ModuleList([])\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (len(in_out) - 1)\n",
    "            up_modules.append(nn.ModuleList([\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_out * 2, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                ConditionalResidualBlock1D(\n",
    "                    dim_in, dim_in, cond_dim=cond_dim,\n",
    "                    kernel_size=kernel_size, n_groups=n_groups),\n",
    "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
    "            ]))\n",
    "\n",
    "        final_conv = nn.Sequential(\n",
    "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
    "            nn.Conv1d(start_dim, input_dim, 1),\n",
    "        )\n",
    "\n",
    "        self.diffusion_step_encoder = diffusion_step_encoder\n",
    "        self.up_modules = up_modules\n",
    "        self.down_modules = down_modules\n",
    "        self.final_conv = final_conv\n",
    "\n",
    "        print(\"number of parameters: {:e}\".format(\n",
    "            sum(p.numel() for p in self.parameters()))\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                sample: torch.Tensor,\n",
    "                timestep: Union[torch.Tensor, float, int],\n",
    "                global_cond=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,input_dim)\n",
    "        timestep: (B,) or int, diffusion step\n",
    "        global_cond: (B,global_cond_dim)\n",
    "        output: (B,T,input_dim)\n",
    "        \"\"\"\n",
    "        # (B,T,C)\n",
    "        sample = sample.moveaxis(-1, -2)\n",
    "        # (B,C,T)\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        global_feature = self.diffusion_step_encoder(timesteps)\n",
    "\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                global_feature, global_cond\n",
    "            ], axis=-1)\n",
    "\n",
    "        x = sample\n",
    "        h = []\n",
    "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        for mid_module in self.mid_modules:\n",
    "            x = mid_module(x, global_feature)\n",
    "\n",
    "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = resnet(x, global_feature)\n",
    "            x = resnet2(x, global_feature)\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # (B,C,T)\n",
    "        x = x.moveaxis(-1, -2)\n",
    "        # (B,T,C)\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# @markdown ### **Vision Encoder**\n",
    "# @markdown\n",
    "# @markdown Defines helper functions:\n",
    "# @markdown - `get_resnet` to initialize standard ResNet vision encoder\n",
    "# @markdown - `replace_bn_with_gn` to replace all BatchNorm layers with GroupNorm\n",
    "\n",
    "def get_resnet(name: str, weights=None, **kwargs) -> nn.Module:\n",
    "    \"\"\"\n",
    "    name: resnet18, resnet34, resnet50\n",
    "    weights: \"IMAGENET1K_V1\", None\n",
    "    \"\"\"\n",
    "    # Use standard ResNet implementation from torchvision\n",
    "    func = getattr(torchvision.models, name)\n",
    "    resnet = func(weights=weights, **kwargs)\n",
    "\n",
    "    # remove the final fully connected layer\n",
    "    # for resnet18, the output dim should be 512\n",
    "    resnet.fc = torch.nn.Identity()\n",
    "    return resnet\n",
    "\n",
    "\n",
    "def replace_submodules(\n",
    "        root_module: nn.Module,\n",
    "        predicate: Callable[[nn.Module], bool],\n",
    "        func: Callable[[nn.Module], nn.Module]) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Replace all submodules selected by the predicate with\n",
    "    the output of func.\n",
    "\n",
    "    predicate: Return true if the module is to be replaced.\n",
    "    func: Return new module to use.\n",
    "    \"\"\"\n",
    "    if predicate(root_module):\n",
    "        return func(root_module)\n",
    "\n",
    "    bn_list = [k.split('.') for k, m\n",
    "               in root_module.named_modules(remove_duplicate=True)\n",
    "               if predicate(m)]\n",
    "    for *parent, k in bn_list:\n",
    "        parent_module = root_module\n",
    "        if len(parent) > 0:\n",
    "            parent_module = root_module.get_submodule('.'.join(parent))\n",
    "        if isinstance(parent_module, nn.Sequential):\n",
    "            src_module = parent_module[int(k)]\n",
    "        else:\n",
    "            src_module = getattr(parent_module, k)\n",
    "        tgt_module = func(src_module)\n",
    "        if isinstance(parent_module, nn.Sequential):\n",
    "            parent_module[int(k)] = tgt_module\n",
    "        else:\n",
    "            setattr(parent_module, k, tgt_module)\n",
    "    # verify that all modules are replaced\n",
    "    bn_list = [k.split('.') for k, m\n",
    "               in root_module.named_modules(remove_duplicate=True)\n",
    "               if predicate(m)]\n",
    "    assert len(bn_list) == 0\n",
    "    return root_module\n",
    "\n",
    "\n",
    "def replace_bn_with_gn(\n",
    "        root_module: nn.Module,\n",
    "        features_per_group: int = 16) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Relace all BatchNorm layers with GroupNorm.\n",
    "    \"\"\"\n",
    "    replace_submodules(\n",
    "        root_module=root_module,\n",
    "        predicate=lambda x: isinstance(x, nn.BatchNorm2d),\n",
    "        func=lambda x: nn.GroupNorm(\n",
    "            num_groups=x.num_features // features_per_group,\n",
    "            num_channels=x.num_features)\n",
    "    )\n",
    "    return root_module\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# @markdown ### **Network Demo**\n",
    "\n",
    "# construct ResNet18 encoder\n",
    "# if you have multiple camera views, use seperate encoder weights for each view.\n",
    "vision_encoder = get_resnet('resnet18')\n",
    "\n",
    "# IMPORTANT!\n",
    "# replace all BatchNorm with GroupNorm to work with EMA\n",
    "# performance will tank if you forget to do this!\n",
    "vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "\n",
    "# ResNet18 has output dim of 512\n",
    "vision_feature_dim = 512\n",
    "# agent_pos is 2 dimensional\n",
    "lowdim_obs_dim = 2\n",
    "# observation feature has 514 dims in total per step\n",
    "obs_dim = vision_feature_dim + lowdim_obs_dim\n",
    "action_dim = 2\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim * obs_horizon\n",
    ")\n",
    "\n",
    "# the final arch has 2 parts\n",
    "nets = nn.ModuleDict({\n",
    "    'vision_encoder': vision_encoder,\n",
    "    'noise_pred_net': noise_pred_net\n",
    "})\n",
    "\n",
    "# demo\n",
    "with torch.no_grad():\n",
    "    # example inputs\n",
    "    image = torch.zeros((1, obs_horizon, 3, 96, 96))\n",
    "    agent_pos = torch.zeros((1, obs_horizon, 2))\n",
    "    # vision encoder\n",
    "    image_features = nets['vision_encoder'](\n",
    "        image.flatten(end_dim=1))\n",
    "    # (2,512)\n",
    "    image_features = image_features.reshape(*image.shape[:2], -1)\n",
    "    # (1,2,512)\n",
    "    obs = torch.cat([image_features, agent_pos], dim=-1)\n",
    "    # (1,2,514)\n",
    "\n",
    "    noised_action = torch.randn((1, pred_horizon, action_dim))\n",
    "    diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "    # the noise prediction network\n",
    "    # takes noisy action, diffusion iteration and observation as input\n",
    "    # predicts the noise added to action\n",
    "    noise = nets['noise_pred_net'](\n",
    "        sample=noised_action,\n",
    "        timestep=diffusion_iter,\n",
    "        global_cond=obs.flatten(start_dim=1))\n",
    "\n",
    "    # illustration of removing noise\n",
    "    # the actual noise removal is performed by NoiseScheduler\n",
    "    # and is dependent on the diffusion noise schedule\n",
    "    denoised_action = noised_action - noise\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = nets.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# @markdown ### **Training**\n",
    "# @markdown\n",
    "# @markdown Takes about 2.5 hours. If you don't want to wait, skip to the next cell\n",
    "# @markdown to load pre-trained weights\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(\n",
    "    parameters=nets.parameters(),\n",
    "    power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=nets.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=500,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    for epoch_idx in tglobal:\n",
    "        epoch_loss = list()\n",
    "        # batch loop\n",
    "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
    "            for nbatch in tepoch:\n",
    "                # data normalized in dataset\n",
    "                # device transfer\n",
    "                nimage = nbatch['image'][:, :obs_horizon].to(device)\n",
    "                nagent_pos = nbatch['agent_pos'][:, :obs_horizon].to(device)\n",
    "                naction = nbatch['action'].to(device)\n",
    "                B = nagent_pos.shape[0]\n",
    "\n",
    "                # encoder vision features\n",
    "                image_features = nets['vision_encoder'](\n",
    "                    nimage.flatten(end_dim=1))\n",
    "                image_features = image_features.reshape(\n",
    "                    *nimage.shape[:2], -1)\n",
    "                # (B,obs_horizon,D)\n",
    "\n",
    "                # concatenate vision feature and low-dim obs\n",
    "                obs_features = torch.cat([image_features, nagent_pos], dim=-1)\n",
    "                obs_cond = obs_features.flatten(start_dim=1)\n",
    "                # (B, obs_horizon * obs_dim)\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                noise = torch.randn(naction.shape, device=device)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,), device=device\n",
    "                ).long()\n",
    "\n",
    "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_actions = noise_scheduler.add_noise(\n",
    "                    naction, noise, timesteps)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(nets.parameters())\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                epoch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
    "\n",
    "# Weights of the EMA model\n",
    "# is used for inference\n",
    "ema_nets = nets\n",
    "ema.copy_to(ema_nets.parameters())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# %%\n",
    "# @markdown ### **Loading Pretrained Checkpoint**\n",
    "# @markdown Set `load_pretrained = True` to load pretrained weights.\n",
    "\n",
    "load_pretrained = True\n",
    "if load_pretrained:\n",
    "    ckpt_path = \"pusht_vision_100ep.ckpt\"\n",
    "    if not os.path.isfile(ckpt_path):\n",
    "        id = \"1XKpfNSlwYMGaF5CncoFaLKCDTWoLAHf1&confirm=t\"\n",
    "        gdown.download(id=id, output=ckpt_path, quiet=False)\n",
    "\n",
    "    state_dict = torch.load(ckpt_path, map_location='cuda')\n",
    "    ema_nets = nets\n",
    "    ema_nets.load_state_dict(state_dict)\n",
    "    print('Pretrained weights loaded.')\n",
    "else:\n",
    "    print(\"Skipped pretrained weight loading.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:10:55.234634Z",
     "start_time": "2024-07-16T11:10:37.199497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# @markdown ### **Inference**\n",
    "\n",
    "# limit enviornment interaction to 200 steps before termination\n",
    "max_steps = 500\n",
    "env = PushTImageEnv()\n",
    "# use a seed > 200 to avoid initial states seen in the training dataset\n",
    "env.seed(1000230)\n",
    "\n",
    "# get first observation\n",
    "obs, info = env.reset()\n",
    "\n",
    "# keep a queue of last 2 steps of observations\n",
    "obs_deque = collections.deque(\n",
    "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "# save visualization and rewards\n",
    "imgs = [env.render(mode='rgb_array')]\n",
    "rewards = list()\n",
    "done = False\n",
    "step_idx = 0\n",
    "\n",
    "with tqdm(total=max_steps, desc=\"Eval PushTImageEnv\") as pbar:\n",
    "    while not done:\n",
    "        B = 1\n",
    "        # stack the last obs_horizon number of observations\n",
    "        images = np.stack([x['image'] for x in obs_deque])\n",
    "        agent_poses = np.stack([x['agent_pos'] for x in obs_deque])\n",
    "\n",
    "        # normalize observation\n",
    "        nagent_poses = normalize_data(agent_poses, stats=stats['agent_pos'])\n",
    "        # images are already normalized to [0,1]\n",
    "        nimages = images\n",
    "\n",
    "        # device transfer\n",
    "        nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
    "        # (2,3,96,96)\n",
    "        nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n",
    "        # (2,2)\n",
    "\n",
    "        # infer action\n",
    "        with torch.no_grad():\n",
    "            # get image features\n",
    "            image_features = ema_nets['vision_encoder'](nimages)\n",
    "            # (2,512)\n",
    "\n",
    "            # concat with low-dim observations\n",
    "            obs_features = torch.cat([image_features, nagent_poses], dim=-1)\n",
    "\n",
    "            # reshape observation to (B,obs_horizon*obs_dim)\n",
    "            obs_cond = obs_features.unsqueeze(0).flatten(start_dim=1)\n",
    "\n",
    "            # initialize action from Guassian noise\n",
    "            noisy_action = torch.randn(\n",
    "                (B, pred_horizon, action_dim), device=device)\n",
    "            naction = noisy_action\n",
    "\n",
    "            # init scheduler\n",
    "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "            for k in noise_scheduler.timesteps:\n",
    "                # predict noise\n",
    "                noise_pred = ema_nets['noise_pred_net'](\n",
    "                    sample=naction,\n",
    "                    timestep=k,\n",
    "                    global_cond=obs_cond\n",
    "                )\n",
    "\n",
    "                # inverse diffusion step (remove noise)\n",
    "                naction = noise_scheduler.step(\n",
    "                    model_output=noise_pred,\n",
    "                    timestep=k,\n",
    "                    sample=naction\n",
    "                ).prev_sample\n",
    "\n",
    "        # unnormalize action\n",
    "        naction = naction.detach().to('cpu').numpy()\n",
    "        # (B, pred_horizon, action_dim)\n",
    "        naction = naction[0]\n",
    "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
    "\n",
    "        # only take action_horizon number of actions\n",
    "        start = obs_horizon - 1\n",
    "        end = start + action_horizon\n",
    "        action = action_pred[start:end, :]\n",
    "        # (action_horizon, action_dim)\n",
    "\n",
    "        # execute action_horizon number of steps\n",
    "        # without replanning\n",
    "        for i in range(len(action)):\n",
    "            # stepping env\n",
    "            obs, reward, done, _, info = env.step(action[i])\n",
    "            # save observations\n",
    "            obs_deque.append(obs)\n",
    "            # and reward/vis\n",
    "            rewards.append(reward)\n",
    "            imgs.append(env.render(mode='rgb_array'))\n",
    "\n",
    "            # update progress bar\n",
    "            step_idx += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(reward=reward)\n",
    "            if step_idx > max_steps:\n",
    "                done = True\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "# print out the maximum target coverage\n",
    "print('Score: ', max(rewards))\n",
    "# list -> numpy\n",
    "imgs = np.array(imgs)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eval PushTImageEnv:   0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54b2e90c041d4c0c927188629bd94407"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.009091382559416949\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:11:00.515541Z",
     "start_time": "2024-07-16T11:11:00.499965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# list -> numpy\n",
    "imgs = np.array(imgs)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T10:18:13.347668Z",
     "start_time": "2024-07-16T10:18:13.242703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import skvideo\n",
    "# skvideo.setFFmpegPath(r\"E:\\ffmpeg\\bin\")\n",
    "# from skvideo.io import vwrite\n",
    "# # visualize\n",
    "# vwrite('vis.mp4', imgs)\n",
    "# Video('vis.mp4', embed=True, width=256, height=256)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Video object>"
      ],
      "text/html": [
       "<video controls  width=\"256\"  height=\"256\">\n",
       " <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T11:11:05.310099Z",
     "start_time": "2024-07-16T11:11:05.285372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 设置输出视频的路径和帧率\n",
    "output_video_path = \"output_video.avi\"\n",
    "frame_rate = 30  # 每秒30帧\n",
    "# 获取图像的宽度和高度\n",
    "height, width = imgs.shape[1:3]\n",
    "# 创建 VideoWriter 对象\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (width, height))\n",
    "# 写入每一帧到视频\n",
    "for frame in imgs:\n",
    "    out.write(frame)\n",
    "# 释放 VideoWriter\n",
    "out.release()\n",
    "print(\"Video has been created at\", output_video_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video has been created at output_video.avi\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": []
  }
 ]
}
